# ğŸ¥ Medical AI Service

Intelligent medical assistant for symptom analysis and specialist doctor recommendations.

## ğŸš€ Features

- **Symptom Analysis** - AI-powered recognition of medical complaints
- **Doctor Recommendations** - suggests appropriate specialists based on symptoms
- **Multilingual Support** - Russian and English languages
- **Interactive CLI** - console chat interface with the assistant
- **Local Processing** - works with Ollama without sending data to the cloud
- **Performance Optimized** - fast responses with caching and graceful degradation
- **Production Ready** - comprehensive testing, Docker support, health checks

## ğŸ› ï¸ Technologies

- **Python 3.8+**
- **LangChain** - framework for working with LLM
- **Ollama** - local language model execution
- **Llama 3.2:3b** - optimized language model
- **Clean Architecture** - proper separation of concerns
- **Singleton Pattern** - optimized model loading

## ğŸ“‹ Supported Symptoms

100+ symptoms mapped to appropriate specialists:

| Category | Examples | Recommended Doctors |
|----------|----------|--------------------|
| Neurological | Headache, dizziness, weakness | Neurologist, General Practitioner |
| Respiratory | Cough, shortness of breath | Pulmonologist, General Practitioner |
| Gastrointestinal | Abdominal pain, nausea | Gastroenterologist |
| Cardiovascular | Chest pain, heart palpitations | Cardiologist |
| Dermatological | Rash, itching | Dermatologist |
| Ophthalmological | Eye problems, vision issues | Ophthalmologist |
| Dental | Toothache, gum problems | Dentist |
| ENT | Throat pain, ear problems | Otolaryngologist |

## ğŸ”§ Installation

### 1. Install Ollama

```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# Download from https://ollama.ai/download
```

### 2. Download the model

```bash
ollama pull llama3.2:3b-instruct-q4_0
```

### 3. Clone the repository

```bash
git clone https://github.com/aleks2008-dev/medical-ai-service.git
cd medical-ai-service
```

### 4. Create virtual environment

```bash
python -m venv venv
source venv/bin/activate  # macOS/Linux
# or
venv\Scripts\activate     # Windows
```

### 5. Install dependencies

```bash
pip install -r requirements.txt
```

## ğŸš€ Usage

### 1. Start Ollama

```bash
ollama serve
```

### 2. Run the assistant

```bash
python main.py
```

## ğŸ’¬ Usage Examples

```
Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ: Ğ£ Ğ¼ĞµĞ½Ñ Ğ±Ğ¾Ğ»Ğ¸Ñ‚ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ° Ğ¸ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ°
ĞÑ‚Ğ²ĞµÑ‚: ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ñ, Ñ‡Ñ‚Ğ¾ Ğ²Ğ°Ğ¼ Ğ½ĞµÑ…Ğ¾Ñ€Ğ¾ÑˆĞ¾. ĞŸÑ€Ğ¸ Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ±Ğ¾Ğ»Ğ¸ Ğ¸ Ñ‚ĞµĞ¼Ğ¿ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒÑÑ Ğº Ñ‚ĞµÑ€Ğ°Ğ¿ĞµĞ²Ñ‚Ñƒ Ğ¸Ğ»Ğ¸ Ğ½ĞµĞ²Ñ€Ğ¾Ğ»Ğ¾Ğ³Ñƒ. ĞĞ½Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ÑƒÑ‚ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾Ğµ Ğ¾Ğ±ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡Ğ°Ñ‚ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ÑÑ‰ĞµĞµ Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ.

Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ: ĞšĞ°ÑˆĞµĞ»ÑŒ ÑƒĞ¶Ğµ Ğ½ĞµĞ´ĞµĞ»Ñ
ĞÑ‚Ğ²ĞµÑ‚: ĞŸÑ€Ğ¸ Ğ´Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ ĞºĞ°ÑˆĞ»Ğµ ÑĞ»ĞµĞ´ÑƒĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒÑÑ Ğº Ğ¿ÑƒĞ»ÑŒĞ¼Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³Ñƒ Ğ¸Ğ»Ğ¸ Ñ‚ĞµÑ€Ğ°Ğ¿ĞµĞ²Ñ‚Ñƒ Ğ´Ğ»Ñ Ğ´Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ¸ Ğ¸ Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ.

Question: Thank you for your help!
Answer: You're welcome! Take care of yourself and don't delay visiting a doctor when necessary.
```

## âš™ï¸ Configuration

Model settings in `src/config/settings.py`:

```python
MODEL_NAME = "llama3.2:3b-instruct-q4_0"
MODEL_PROVIDER = "ollama"
MODEL_TEMPERATURE = 0
OLLAMA_BASE_URL = "http://localhost:11434"
MODEL_NUM_CTX = 512  # Optimized context
MODEL_NUM_PREDICT = 192  # Balanced response length
```

## ğŸ—ï¸ Architecture

### Clean Architecture Implementation
```
src/
â”œâ”€â”€ config/           # Configuration layer
â”œâ”€â”€ models/           # Domain models
â”œâ”€â”€ services/         # Business logic
â””â”€â”€ utils/           # Infrastructure
```

### Key Features
- **Singleton Pattern** - Model loads once per session
- **Caching** - Responses cached for performance
- **Rate Limiting** - Graceful degradation under load
- **Health Checks** - Lightweight system monitoring
- **Error Handling** - Comprehensive exception management

## ğŸ§ª Testing

```bash
# Run all tests
python -m pytest tests/ -v

# Run with coverage
python -m pytest tests/ --cov=src --cov-report=html
```

## ğŸ³ Docker Support

```bash
# Build and run
docker-compose up --build

# Or run with Docker
docker build -t medical-ai-service .
docker run -it medical-ai-service
```

## ğŸ“Š Performance

- **Response Time**: 2-5 seconds (optimized)
- **Memory Usage**: ~200MB (with model loaded)
- **Supported Load**: 10 requests/minute per user
- **Cache Hit Rate**: ~30% for common symptoms

## ğŸ” Project Structure

```
medical-ai-service/
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ config/            # Configuration
â”‚   â”‚   â””â”€â”€ settings.py    # Application settings
â”‚   â”œâ”€â”€ models/            # Data models
â”‚   â”‚   â””â”€â”€ symptom_data.py # Symptom-doctor mapping
â”‚   â”œâ”€â”€ services/          # Business logic
â”‚   â”‚   â”œâ”€â”€ ai_service.py  # AI service (Singleton)
â”‚   â”‚   â””â”€â”€ doctor_service.py # Doctor recommendations
â”‚   â””â”€â”€ utils/             # Utilities
â”‚       â”œâ”€â”€ cli.py         # CLI interface
â”‚       â””â”€â”€ health.py      # Health checks
â”œâ”€â”€ tests/                 # Test suite
â”œâ”€â”€ main.py               # Application entry point
â”œâ”€â”€ requirements.txt      # Dependencies
â”œâ”€â”€ Dockerfile           # Container configuration
â”œâ”€â”€ docker-compose.yaml  # Multi-service setup
â””â”€â”€ README.md           # Documentation
```

## âš ï¸ Important Notes

- **Medical Disclaimer** - This tool is for initial consultation only, not a replacement for professional medical advice
- **Privacy** - All processing is local, no data sent to external services
- **Requirements** - Requires Ollama service running locally
- **Languages** - Supports Russian and English

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

MIT License - see LICENSE file for details

## ğŸ†˜ Troubleshooting

Common issues and solutions:

1. **Ollama not running**
   ```bash
   ollama serve
   ```

2. **Model not found**
   ```bash
   ollama pull llama3.2:3b-instruct-q4_0
   ```

3. **Port conflicts**
   - Check if port 11434 is available
   - Modify OLLAMA_BASE_URL in .env if needed

4. **Performance issues**
   - Reduce MODEL_NUM_CTX in settings
   - Close other resource-intensive applications

---

**âš•ï¸ Medical Disclaimer: This assistant provides general information only and does not replace professional medical consultation. Always consult qualified healthcare providers for medical advice.**